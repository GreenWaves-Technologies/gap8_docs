# MNIST image recognition

## Introduction

This document describes a MNINST hand written digit recognition application based on a convolutional neural network (CNN) on the GAP8 processor.
The example can be compiled and executed on the virtual model of GAP8 available in the SDK.

In the example, 28x28 pixels images are captured from a camera model by the camera interface and loaded in the L2 memory of the fabric controller.
The images are then processed by the CNN on the 8 cores of the cluster. The CNN classifies the images in 10 categories (digit from 0 to 9), by selecting the maximum confidence value at the output as the inferred category.

The network topology (number of layers, number of inputs and outputs for the layers) has been taken from an open source project on the Internet. In this example, we demonstrate the implementation 
of this network on GAP8 using adapted data management through DMA and optimized parallelized filter kernels taking benefits of the 8 cores of the cluster.

The CNN has been trained off-line using the MNIST dataset. The recognition ratio reached after training is 96%, which is very close to the asymptotic ratio announced by the authors.
Actually more sophisticated topologies can achieve a better rate, but this example is given more as a simpler tutorial for CNN implementation principles on GAP8 architecture.
Specifically, it demonstrates how to perform data management between L2 memory in the fabric controller and the L1 cluster shared memory so that no memory overflow occurs while keeping all the compute resources busy.
It also shows how the basic filter kernels (2D convolution filter and linear filters) can be described to use the 8 cores of the cluster efficiently so as to reduce computation time and hence power consumption of the application.

In the next sections, we will describe the structure of the example directory, the structure of the CNN and its software components and finally how to run the application on the GAP8 simulator.

## Structure of the directory

In the GAP8 installation repository, the mnist example is located in app_release/applications/mnist/

The directory is composed of the following:

main_cam.c
  : main application code

Makefile
  : compilation and execution command file

l2_data.h
  : allocation of data stored in L2

RGB.img
  : one test image in RGB565 format

run_script.sh
  : script to run executions on different test images

inc_gen/L2.h
  : biases and weights for the MNIST CNN quantized with a Q14 representation

test_img
  : 10 28x28 pixels images from the MNIST test dataset

src_lib
  : library of kernels and functions for CNNs

CnnKernels.c(h)
  : Functions of the Cnn layers

CnnKernelsInit.ch
  : parameters for the CNN layer functions
  
gen_lib_save
  : Directory for the saved reference generated files


## Description of the CNN

The CNN is composed of three layers, described in the procedure `network_process()` of the main_cam.c file:

***One 2 Dimension convolutional layer with one input 28x28  ReLU and 2x2 max pooling producing 32 output images, each 12x12 samples***

~~~~c
ConvLayer0(l2_x, L2_W_0, l2_big0, 14, L2_B_0, NULL); 
~~~~

Each layer is called a "Kernel" in the following.
A kernel is executed on the cluster. It is launched by the fabric controler, and it encapsulates all the DMA transfers and sync functions to execute the function on the 8 cores of the cluster.
It can be either generated by the autotiler or taken from the gen_lib_save directory

ConvLayer0 is a *kernel* function that manages data transfers and 2D convolutional filter execution on the input l2_x. It is described in the CnnKernels_newrt.c file.

+ l2_x is a pointer to the buffer containing the input image loaded from the camera
+ L2_W_0 is a pointer to the convolution filter kernel coefficients
+ L2_B_0 is a pointer to the biases
+ l2_big0 is a pointer to the buffer containing the results of the 2D convolution filter for all output features

***One 2 Dimension Convolution layer with 32 inputs of 12x12 pixels and 64 outputs 4x4, ReLU and 2x2 max pooling producing 64 outputs of samples***

~~~~c
ConvLayer1(l2_big0, L2_W_2, l2_big1, 14, L2_B_2, NULL);
~~~~

Same as above, with the difference that the input is l2_big0, and the output l2_big1.

***One Dense layer producing 10 outputs by matrix multiply of the input dimension (1x1024) by the weight matrix  (1024x10).***

~~~~c
Dense(l2_big1, L2_W_4, 16, L2_B_4, 13, l2_big0, 10, NULL); 
~~~~

This kernel function manages data transfers and linear filtering of the input l2_big1. It is described in the Cnnkernels_newrt.c file.

+ L2_W_4 points to the weights
+ L2_B_4 points to the biases
+ l2_big0 points to the output
+ AllKernels+2 points to the arguments of the kernel function.

Note that all structures (l2_x, l2_big*, L2_W_*, L2_B_* are located in the L2 memory of the fabric controller), while the kernels are executed on the cluster. Those data are transfered to the L1 memory by the DMA processes described in the ConvLayer and Dense kernel functions.

## Automatic Layer Code Generation

As in the cifar10 example, the low level code for the CNN layers has been generated using the auto-tiler tool available in the SDK.
The generator assumes that the data and weights are stored in the L2 memory (in the Fabric Controller). The available size of L1 shared memory for processing is specified to the auto-tiler.

The data and weights are split into tiles, forming  data sets that will be loaded through DMA to the L1 shared memory of the cluster. The cores will process the data and when finished, the results are stored back into the L2.
The tool takes the "high level" parameters of the layers as input and generates the low level functions describing the data management and the run-time functions to execute the layers in parallel on the eight cores of the cluster.

In the cifar10 example, a single function was generated for all instances, and the parameters for each layer was passed to the function.
In MNIST example, a different version is generated for each instance, and the parameters are hard coded in the function.

The following files are automatically generated:
- CnnKernels.c	    : contains the generated code for the layers, with the calls to run time DMA process and processor synchronization
- CnnKernel.h	    : contains the header for Convlayer and dense layer functions
- CnnKernelInit.c   : contains the parameters for the DMA transfers and processing, including the tiling management and pipeline processing
- CnnKernelsInit.h  : contains data-type definition for the parameter structures

### How to automatically generate the MNIST CNN code with tiling 

The input file containing the "high level" description of the layers used in MNIST is Model_kernel.c.
Following is the description of the different sections in the input file:


~~~~~c
  SetInlineMode(ALWAYS_INLINE);
~~~~~

This instruction specifies that a  function will be generated for each occurrence of the layer.
To generate a single parameterized function, the SetInlineMode function must be called with "SINGLE_INLINE" parameter.

The following instructions specify the name of the library header files (SetUsedFilesNames), the names of the generated files (SetGeneratedFilesNames), and the declaration of the amount of L1 memory to be used by the tiler generator.

~~~~~c
  SetUsedFilesNames("KernelLibStdTypes.h", 1, "CNN_BasicKernels.h");
  SetGeneratedFilesNames("CnnKernelsInit.c", "CnnKernelsInit.h", "CnnKernels.c", "CnnKernels.h");
  
  SetL1MemorySize(L1Memory);
~~~~~

The kernel libraries are loaded for usage by the tiler:

~~~~~c
  CNN_LoadSoftwareKernelLibrary();
  CNN_LoadHWCEKernelLibrary();
~~~~~

The layer parameters are defined by the code below: 

~~~~~c
  // MNIST config 
  CNN_TiledConvNxNReLUPool2x2_SW_fp("ConvLayer", 5, 1, 32, 28, 28, 3);
  CNN_TiledConvNxNReLUPool2x2_SW_fp("ConvLayer", 5, 32, 64, 12, 12, 3);
  CNN_TiledLinearLayer     ("Dense", 64, 4, 4, 10, 1, 0, 0);
~~~~~

CNN_TiledConvNxNReLUPool2x2_SW_fp : defines a 2D Convolution layer with ReLU and pooling. The parameters are, in order:

+ "ConvLayer": The name of the generated function 
+  5: size of the kernel (3x3, 5x5 or 7x7)
+  1: number of input feature
+ 32: number of output features
+ 28: width of the input image
+ 28: Height of the input image
+  3: ReLUPooling combination (0=>ReLU poolmax,1=>ReLU pool average, 2=> No ReLU Poolmax, 3=> NoRELu PoolAvg)

CNN_TiledLinearLayer : defines a Linear layer (dense layer). The parameters are, in order:

+ "Dense" : The name of the generated function
+ 64: number of input features
+  4: width of the input image
+  4: Height of the input image
+ 10: Number of output features
+  1: Size of input data (0=> 8 bits, 1=> 16bits, 2=> 32bits)
+  0: Relu (0=> not activated)
+  0: Activate use of L3 memory for coefficients (0=> not activated)

### File Generation

In order to generate the CNN kernel files, type the following commands:

define the PATH to auto-tiler libraries and generator

~~~~~sh
export TILE_SRC=../../../auto-tiler/source
~~~~~

Generate the tiler (using as input the CNN description in file Model_kernel.c)

~~~~~sh
gcc -o GenTile -I$TILE_SRC/GenTiling -I$TILE_SRC/StdTypes -I$TILE_SRC/CnnStdModel Model_kernel.c $TILE_SRC/CnnStdModel/CNN_Generator.c $TILE_SRC/GenTiling/LibTile.a
~~~~~

Execute the tiler

~~~~sh
./GenTile
~~~~

In addition to the CnnKernel files, the tiler will produce the following output:

~~~~~text
Loading Lib Kernel                                  KerSetInBias
Loading Lib Kernel                                 KerConv3x3_fp
Loading Lib Kernel                           KerDirectConv3x3_fp
Loading Lib Kernel                                 KerConv5x5_fp
Loading Lib Kernel                           KerDirectConv5x5_fp
Loading Lib Kernel                                 KerConvNxN_fp
Loading Lib Kernel                           KerDirectConvNxN_fp
Loading Lib Kernel                                    KerReLU_fp
Loading Lib Kernel                              KerMaxPool2x2_fp
Loading Lib Kernel                              KerAvgPool2x2_fp
Loading Lib Kernel                          KerReLUMaxPool2x2_fp
Loading Lib Kernel                        KerReLUMaxPool2x2_2_fp
Loading Lib Kernel                        KerReLUAvgPool2x2_2_fp
Loading Lib Kernel                   KerReLUMaxPool2x2_2_Even_fp
Loading Lib Kernel                   KerReLUMaxPool2x2_2_Drop_fp
Loading Lib Kernel                        KerReLUMaxPool2x2_3_fp
Loading Lib Kernel                        KerReLUAvgPool2x2_3_fp
Loading Lib Kernel                            KerLinearLayer_fps
Loading Lib Kernel                             KerLinearLayer_fp
Loading Lib Kernel                            KerLinearLayer_fpd
Loading Lib Kernel                        KerLinearLayerReLU_fps
Loading Lib Kernel                         KerLinearLayerReLU_fp
Loading Lib Kernel                        KerLinearLayerReLU_fpd
Loading Lib Kernel                                   HWCE_Enable
Loading Lib Kernel                                  HWCE_Disable
Loading Lib Kernel                              HWCE_GenericInit
Loading Lib Kernel                               HwCE_SetYinMode
Loading Lib Kernel                                 KerSetInBias2
Loading Lib Kernel                                 KerSetInBias3
Loading Lib Kernel               HWCE_ProcessOneTile3x3_MultiOut
Loading Lib Kernel                        HWCE_ProcessOneTile5x5
Loading Lib Kernel                        HWCE_ProcessOneTile7x7
Loading Lib Kernel                        HWCE_ProcessOneTile7x4
Kernel:           ConvLayer0, Total Raw Memory: 13536 fits into L1 memory 50000. Promoting all kernel arguments to initialized buffers.
Kernel:           ConvLayer1, Arg: Filter (IN_DB_3D_4D) can promote to partial buffer [100 -> 3200]. TotalMem: 50000. Used Mem 872 -> 3972
Kernel:                Dense, Total Raw Memory: 22568 fits into L1 memory 50000. Promoting all kernel arguments to initialized buffers.

Generating User Kernel                                ConvLayer0 [ 1x  28x  28x32]. Use Kernel Descriptor: 0 (user kernel is inlined)
Generating User Kernel                                ConvLayer1 [32x  12x  12x64]. Use Kernel Descriptor: 0 (user kernel is inlined)
Generating User Kernel                                     Dense [ 1x   4x   4x 1]. Use Kernel Descriptor: 0 (user kernel is inlined)

Shared L1 Memory size (Bytes)             : Given: 50000, Used: 22568
L2 Memory size (Bytes)                    : 0
C Symbol for shared L1 Memory             : L1_Memory. Static, defined in CnnKernelsInit.c
C Symbol for L2 Memory                    : L2_Memory. Static, defined in CnnKernelsInit.c
C Symbol for kernels descriptors          : AllKernels
C Symbol for kernels arguments descriptors: AllKernelsArgs
Standard data types used by kernels       : KernelLibStdTypes.h
Basic kernels library                     : CNN_BasicKernels.h
Output Directory                          : .

The following files have been generated:
	                  CnnKernels.c Generated C code for the user kernels and the user kernels groups
	                  CnnKernels.h Header file for the generated C code
	              CnnKernelsInit.c Kernel descriptors constant initialization, shared L1 memory C definition
	              CnnKernelsInit.h Header file for kernel descriptors constant initialization

==> end of the output
~~~~~

The generated files using the actual Model_Kernel present in the MNIST directory are saved as reference in the gen_lib_save directory, so they can be restored from here.

## Compile and run the application

### Single execution

To execute the application on the specific image contained in RGB.img, just type the following in the MNIST directory:

~~~~~sh
	make clean all run platform=gvsoc
~~~~~

The output of the program is a set of 10 soft confidence values, one for each category. The max value is the category detected by the CNN.

~~~~text
open cam
cam started
cam stopped
cluster master start
CNN  running on 8 cores


 ============> cycles 1460611

 feat 0: -14992  
  feat 1: -7216  
  feat 2: -3372  
  feat 3: 5053  
  feat 4: -11588  
  feat 5: 2548  
  feat 6: -12637  
  feat 7: -7197  
  feat 8: -6636  
  feat 9: -7206  
  found max  3
	 Number of active cores 8
~~~~

### Batch execution

To execute the application on the test set contained in test_img, just type the following:

~~~~~sh
	source script_run.sh
~~~~~

This will generate a report where the ground truth label the found label and the number of cycles for execution are shown for each run.
